// Default config file.
// Set values here, then override them in custom configs by including this at
// the top:
// my_experiment.conf:
//   include "defaults.conf"

// Paths and Logging //
proj_path = ../proj_0				//The base directory for project and preprocessed data
exp_number = 0													// Experiment name, will be a subdirectory of project_dir.
                                								// This directory will contain all run directories
run_number = 0													// Run name, will be a subdirectory of exp_name. 

																// This directory will contain logs and ouput
data_path = ../data/demo.txt				// The path of input txt
remote_log_name = ${exp_number}"__"${run_number}

// You can safely ignore these, and they'll be set automatically:
exp_dir = ${proj_path}"/"${exp_number}"/"
run_dir = ${proj_path}"/"${exp_number}"/"${run_number}"/"
local_log_path = ${run_dir}"/log.log"


// Preprocess Settings //
load_preprocess = True
load_preptrain = Falsed
max_seq_length = 20						// Max number of tokens put into model for a tweet (l in paper)
max_word_freq = 10000000000

// Model Parameters //
load_saved_model = False				// Whether to load saved model
do_pretrain = False 					// Whether to do pretrain model
batch_size = 64							// Size of minibatch
num_epochs = 20							// Number of epochs
embedding_dim = 64						// Embedding dimension (d in paper)
window_size = 5							// K in the paper
hidden_size = 1000						// H in the paper
margin = 0.1							// Margin of WARP loss
max_iter = 1000							// Max number of iteration in finding a negative pair
freeze_layer = False ###	
retrieve_k = 20							// Number of predictions to retrieve during evaluation
learning_rate = 0.001 					// Learning rate for Adam optimizer



